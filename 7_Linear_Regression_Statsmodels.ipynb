{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4b085b0-b39a-4d5c-8eb8-baa9142db4b6",
   "metadata": {},
   "source": [
    "# Linear Regression with Statsmodels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4c605f3",
   "metadata": {},
   "source": [
    "In the first notebooks of this repository, we have worked with the library `scikit-learn` to train linear regression models. In this notebook we want to introduce another library that can be used for this purpose: `statsmodels`. \n",
    "\n",
    "The question which library to choose is closely related to your objective. Here are some of the advantages and areas of application for both libraries:\n",
    "\n",
    "**scikit-learn:**\n",
    "\n",
    "* offers wide variety of models and algorithms not only for regression but also for classification, dimensionality reduction, clustering, etc. \n",
    "* clean and easy to learn syntax \n",
    "* emphasis on predictive modeling rather than descriptive statistics \n",
    "* offers tools for transformation pipelines\n",
    "\n",
    "\n",
    "**statsmodels:**\n",
    "\n",
    "* versatile module for complex statistics\n",
    "* offers powerful statistics and econometric tools \n",
    "* good for fitting statistical models, conducting statistical test or data exploration\n",
    "* emphasis on understanding relevant variables and their effect size\n",
    "* useful for time series \n",
    "\n",
    "\n",
    "\n",
    "As you can see both libraries have their advantages. In a nutshell, if you want to analyze your data (e.g. check the statistical significance of different variables for your model), stasmodels is the right choice. If your focus is on fitting a good model and making predictions, scikit-learn is the better choice.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99889c16",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4329c9ab",
   "metadata": {},
   "source": [
    "In this notebook you will learn…\n",
    "\n",
    "* how to use statsmodels for training a linear regression model.\n",
    "* how to interpret the most important metrics of the model summary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a70b62f9-a79a-4759-be38-41cad9c85cbd",
   "metadata": {},
   "source": [
    "## The Cars Dataset\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff201174",
   "metadata": {},
   "source": [
    "We will stick to the at this point hopefully well known cars dataset to train a simple and a multiple linear regression with statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2482ef7-ce76-4d18-94cd-e0e907283ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc87a21-197b-43ff-87c5-17f794e6ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same car dataset as before\n",
    "cars = pd.read_csv(\"data/cars_multivariate.csv\")\n",
    "cars.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e78ab95a-9884-48e2-a6f9-20a39fbcd1cb",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "221f99a7",
   "metadata": {},
   "source": [
    "\n",
    "For our simple linear regression we will model our target variable `mpg` with the feature `horsepower`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f9c30-79e7-4d6e-99f9-1ae5441b3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between horsepower and mpg \n",
    "cars.plot('horsepower', 'mpg', kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d0ab9-2588-4cad-b9d1-676a09a0617f",
   "metadata": {},
   "source": [
    "As usually we need to prepare our data by defining the target and feature(s). In contrast to `scikit-learn`, `statsmodels` does not add a constant term for calculating the intercept per default. If you check the documentation for [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), you will see that one of its parameters is called `fit_intercept` and its default value equals `True`. When using [statsmodels OLS](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html) we need to manually add a constant term, if we want our model to be fitted with an intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039632a-e95c-4a41-93d6-10af6de86a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and feature and add constant term\n",
    "X = cars.horsepower\n",
    "X = sm.add_constant(X)\n",
    "y = cars.mpg\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46321d-659d-4e00-9891-b044df3781a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model and print summary\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print intercept and coefficients\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65496e-70b4-47b5-9cb2-11f02b3cf41a",
   "metadata": {},
   "source": [
    "As you can see the syntax is slightly different, but it yields the same results. One function that makes `statsmodel` incredibly useful for statistic analyses, is the `.summary` which you can print for each model. It gives you an overview over (nearly) all imaginable metrics and statistical figures you might need. At the beginning this can be pretty overwhelming.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff56542-e48a-4fa4-9ea9-9ce4eff1e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print extensive summary\n",
    "results.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b18557c1-fe0c-4c50-ba28-6870ff75959d",
   "metadata": {},
   "source": [
    "### Model Interpretation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d1e174e",
   "metadata": {},
   "source": [
    "We will only briefly talk about this summary. For a more extensive explanation of all of the metrics within the model summary check the end of the notebook.\n",
    "\n",
    "<center><img src=\"images/ols_summary.png\" height=\"400\"/></center>\n",
    "\n",
    "* The blue part of the table gives some specifics on the data and the model itself.\n",
    "* The orange part provides information about the goodness of fit.\n",
    "    * **$R^2$** \n",
    "    * **adjusted $R^2$**\n",
    "* The green part gives information about the estimated coefficients of our model. \n",
    "    * **coef** are the value for intercept, which is the coefficient for the constant, as well as coefficients for each feature \n",
    "    * **P>|t|** is one of the most important statistics in the summary. It uses the t-statistic to produce the p-value, a measurement of how likely your coefficient is measured through our model by chance.\n",
    "* The turquoise part on the bottom provides information about the residuals, autocorrelation and multicollinearity. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9f6f47a-5d93-47a1-9784-567bbacfb07d",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c65fe0cf",
   "metadata": {},
   "source": [
    "Also with `statsmodel` we can train a multiple linear regression with several features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c33929-66a3-4f6e-ae26-380d248ba337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features and add a constant term \n",
    "X2 = cars[['horsepower', 'weight']]\n",
    "X2 = sm.add_constant(X2)\n",
    "y2 = cars.mpg\n",
    "\n",
    "X2.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ab2b2-eb79-449d-ad43-4c536d6dbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also combine all steps using dot notation\n",
    "sm.OLS(y2, X2).fit().summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9121ccf-a588-450f-aa81-5a98f572a357",
   "metadata": {},
   "source": [
    "### Predicting for new observations\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfe0cacb",
   "metadata": {},
   "source": [
    "Like `scikit-learn` `statsmodels` also has a `.predict` method to calculate values for new instances. Let's say we want to predict the `mpg` for a new car with `horsepower` of 250 and a `weight` of 3000. \n",
    "We can input our new data as a list but we need to make sure to add a constant of 1 for the intercept calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fa908-1667-46a5-9bb9-659b4ead6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test car\n",
    "test_car = [1, 200, 3500]\n",
    "sm.OLS(y2, X2).fit().predict(test_car)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d429ca6-640b-423f-9511-f72b14953457",
   "metadata": {},
   "source": [
    "So according to our model a car with the above mentioned characteristics, would have a fuel efficiency of ~15.9 mpg. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e9b2cbe-fee5-4b65-9d91-156e1641f685",
   "metadata": {},
   "source": [
    "## The Formula Notation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e10f915",
   "metadata": {},
   "source": [
    "Besides the syntax we've used above, we can also choose the so-called **formula notation**. It will yield the same results, only the code is different, it slightly resembles the R syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d52190-45e5-4c4f-9668-be8b3cf6c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mpg explained by horsepower and weight\n",
    "smf.ols(formula='mpg ~ horsepower + weight', data=cars).fit().summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21850b79-c3d1-4ae7-9671-c89bc4ab6ef9",
   "metadata": {},
   "source": [
    "## Model Summary - Clearly explained...\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ab82949",
   "metadata": {},
   "source": [
    "Here is a brief description of the summary table. Don't worry, you don't have to know or understand them all right now.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8c5d02d",
   "metadata": {},
   "source": [
    "#### The blue part of the table gives some specifics on the data and the model:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e44f8d79",
   "metadata": {},
   "source": [
    "* **Dep. Variable**: Dependent variable (target)\n",
    "* **Model**: Technique used, an abbreviated version of the method\n",
    "* **Method**: The loss function optimized in the parameter selection process. Least Squares since it picks the parameters that reduce the training error. This is also known as Mean Square Error [MSE].\n",
    "* **No. Observations**: Number of observations used to train the model / size of the training data\n",
    "* **Df Residuals**: Degrees of freedom of the residuals, which is the number of observations – number of parameters. Intercept is a parameter. The purpose of Degrees of Freedom is to reflect the impact of descriptive/summarizing statistics in the model, which in regression is the coefficient. Since the observations must \"live up\" to these parameters, they only have so many free observations, and the rest must be reserved to \"live up\" to the parameters' prophecy. This internal mechanism ensures that there are enough observations to match the parameters.\n",
    "* **Df Model**: Degrees of freedom model numbers our predicting variables.  The number of parameters in the model (not including the constant/intercept term if present)\n",
    "* **Covariance Type**: Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process. Since this model is Ordinary Least Squares, it is non-robust and therefore highly sensitive to outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23baa43d",
   "metadata": {},
   "source": [
    "#### The orange part of the table shows the goodness of fit:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d5c3d6",
   "metadata": {},
   "source": [
    "* **R-squared**: The coefficient of determination. It quantifies the percent of variance in the target variable that is explained by the model. The remaining percentage represents the variance explained by error, the E-term, the part that model and predictors fail to grasp.\n",
    "* **Adj. R-squared**: Version of the R-Squared that penalizes additional independent variables.\n",
    "* **F-statistic**: A measure of how significant the fit is. The mean squared error of the model divided by the mean squared error of the residuals. Feeds into the calculation of the P-Value.\n",
    "* **Prob (F-statistic) or P-Value**: The probability that a sample like this would yield the above statistic, and whether the model's verdict on the null hypothesis will consistently represent the population. Does not measure effect magnitude, instead measures the integrity and consistency of this test on this group of data.\n",
    "* **Log-likelihood**: \n",
    "* **AIC**: The Akaike Information Criterion. Adjusts the log-likelihood based on the number of observations and the complexity of the model. Penalizes the model selection metrics when more independent variables are added.\n",
    "* **BIC**: The Bayesian Information Criterion. Similar to the AIC, but has a higher penalty for models with more parameters. Penalizes the model selection metrics when more independent variables are added.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3582399d",
   "metadata": {},
   "source": [
    "#### The green second table shows the coefficient report:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "140e4761",
   "metadata": {},
   "source": [
    "* **coef**: The estimated value of the coefficient. By how much the model multiplies the independent value by.\n",
    "std err: The basic standard error of the estimate of the coefficient. Average distance deviation of the points from the model, which offers a unit relevant way to gauge model accuracy.\n",
    "* **t**: The t-statistic value. This is a measure of how statistically significant the coefficient is.\n",
    "* **P > |t|**: P-value that the null-hypothesis that the coefficient = 0 is true. If it is less than the confidence level, often 0.05, it indicates that there is a statistically significant relationship between the term and the response.\n",
    "* **[95.0% Conf. Interval]**: The lower and upper values of the 95% confidence interval. Specific range of the possible coefficient values.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e011366",
   "metadata": {},
   "source": [
    "#### The turquoise third table provides information about residuals, autocorrelation, and multicollinearity:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec741368",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **Skewness**: A measure of the symmetry of the data about the mean. Normally-distributed errors should be symmetrically distributed about the mean (equal amounts above and below the line). The normal distribution has 0 skew.\n",
    "* **Kurtosis**: A measure of the shape of the distribution. Compares the amount of data close to the mean with those far away from the mean (in the tails), so model \"peakiness\". The normal distribution has a Kurtosis of 3, and the greater the number, the more the curve peaks.\n",
    "* **Omnibus D’Angostino’s test**: It provides a combined statistical test for the presence of skewness and kurtosis.\n",
    "* **Prob(Omnibus)**: The above statistic turned into a probability\n",
    "* **Jarque-Bera**: A different test of the skewness and kurtosis\n",
    "* **Prob (JB)**: The above statistic turned into a probability\n",
    "* **Durbin-Watson**: A test for the presence of autocorrelation (that the errors are not independent), which is often important in time-series analysis. It is also a measurement of homoscedasticity. A perfect value would be between 1 and 2. \n",
    "* **Cond. No**: Condition number; A test for multicollinearity (if in a fit with multiple parameters, the parameters are related to each other). Multicollinearity is strongly implied by a high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d18515",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
